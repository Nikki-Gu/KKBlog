---
title: 激活函数和损失函数
order: 4

copyright: <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0协议</a>

---



二分类：sigmoid，BCE Loss

多分类：softmax，CE Loss

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YmVkOWVjNjA0YTE3YjJiMWE2ZTNkYThiMWQyMjM2YWRfRlFtYjlGb1FTSnJ0MXlCbFo1OUNVRE1yc0dqUjdXM0pfVG9rZW46WGtmM2JObmlhb3Z6czN4VDcyZmNsWEg5bnNoXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA)

Pytorch中的使用这两个损失函数

```JSON
// BCE Loss 输入需要先手动进行sigmoid计算再输入
torch.nn.BCELoss()

// softmax + CE Loss
torch.nn.CrossEntropyLoss()
```

最后一层全连接层的神经元个数不同，含义不同。二分类是一个神经元输出概率值（是/不是目标），多分类是多个神经元，每个神经元是一个分布

## Sigmoid

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjQ0YjI4MTg3Y2I5MGExMTYwYmJkNGNmMmNjNzQxZWRfdDk5R3ZoTlNNSTFuUDVhRmJFMFBLclh6QkNBQXJoTEhfVG9rZW46T1dpM2JOTEpCb0Z2aFd4QjZvZ2MwbTVvbkRnXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA)

Sigmoid 函数具有以下特点：

- 单调递增：输入值增加时，Sigmoid 函数的输出值也会增加。
- 连续可导：Sigmoid 函数在定义域内处处可导，便于使用梯度下降等优化算法进行训练。
- 易于计算：Sigmoid 函数的计算相对简单，只涉及指数函数和除法操作。

缺点：

- 梯度消失：函数的输出并不以零为中心；在输入值较大或较小时，梯度可能会消失

求导：

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YzkyZmQyNjgwMzU0ZGZmMDM4ZjIxY2E5ODZhZjFhN2NfWThSMDh1MjRwVTk4dlpIVFM1U1NZUlFlV3NUWE4zbEpfVG9rZW46THk5VGI3R0Nqb1BxYkd4UGhDSGNoNGh3blZkXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA)

## Softmax

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjZlYjIyMTg4YWI4MDRkYjBhNDFiN2M5ZGFmMjVkMTNfZmV4OWZ4eDFjOFliZGJFU1VMVXNFYUc0TmFHb2diWFJfVG9rZW46UkI3UWJOWGF4b2x2YnR4ZFFYemNoRm1Wbnh6XzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA)

Softmax 函数具有以下特点：

- 将输入值映射到一个概率分布：常用于对多分类问题进行建模和预测
- 相对大小：Softmax 函数保持了输入值之间的相对大小关系；对于输出的相对大小非常敏感，可以将差距大的数值距离拉的更大
- 平滑性：Softmax 函数的输出是连续的，并且对输入值的变化相对平滑，因此在训练过程中可以更好地利用梯度信息

### 求导

指对xj求导，分为两种情况讨论：

- i=j是指 softmax(xi)对xi求导
- i不等于j是指 softmax(xi)对xj求导

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MGQ3YzZhNTZmNTUzZTdjNzQ0ZWNhNTJhYjRjYjFmMGNfVnozYWpSWUYwU21nalhFblVaa3FuTkt2Z2VjWmJ4RFVfVG9rZW46Szd0WmJOUXVSb3VyMlJ4U1JhSmNoM2tzbk1oXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA)

### 代码实现

```Python
def softmax(x):
    exp_x = torch.exp(x)
    sum_exp_x = torch.sum(exp_x, dim = 1, keepdim=True)
    return exp_x / sum_exp_x
```

### 上溢出和下溢出问题

上溢出：xi指数运算的取值过大，超出精度表示范围

下溢出：xi指数运算的取值很小，分母变为0

解决溢出问题有两种方法

1. #### 变为计算softmax(xi - c)

- c是x中的最大值
- 分子的最大取值变为了exp(0)=1，避免了上溢出；
- 分母中至少会+1，避免了分母为0造成下溢出。

```Python
def softmax_1(x):
    c, _ = torch.max(x, dim=1, keepdim=True)
    exp_x = torch.exp(x - c)
    sum_exp_x = torch.sum(exp_x, dim = 1, keepdim=True)
    return exp_x / sum_exp_x
```

1. #### Log-softmax

定义为：log(softmax(x))

同样需要通过softmax(xi - c)解决上溢出问题

存在下溢出问题：softmax(x)=0的时候，求log返回为-inf 下溢出

好处：减少指数运算次数，求导更方便，可以加快反向传播速度

## Tanh、Relu、Leaky Relu

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=N2YzNWRiZjIzMDlhNmU2NWVlZjA3MzM1MDNjMDMzMWRfRkgyWDZFWHJWdkVhUkRsY2U0NjlGblNRdHBYRmx2aXBfVG9rZW46WTB5Z2I5V25FbzB2WnN4RnU0QmNRWW9wblVjXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA)