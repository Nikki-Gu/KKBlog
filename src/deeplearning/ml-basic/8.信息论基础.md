---
title: 信息论基础
order: 8

copyright: <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0协议</a>
---

## 交叉熵损失的理解

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MTEzYjgxN2UzN2RiMTFmZjk0ZmMzOTEyMzllMmY1OGFfYVY3TDl4eVRnRTJaWHVqYnNXWnphMHgwZlJFeHBkdURfVG9rZW46UkZaSmJKNnZUb2NBZkV4RmtsYWMzVHlXbmVsXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA)

### 信息熵

- entropy：a measure of impurity
- 用来度量信息的混乱程度、不确定度。不确定性越大，信息熵越大

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=NDE1Nzk1MzRjYTZhNmVmMDk3MzA1NTg1MTBiZjNjODRfOER4QU9QcTUxOHVSczZRaHdoenVMV205WnBuMUNTNTlfVG9rZW46S1l5MWJmY29Eb2RLZkR4NGJFbGNGWWFHblRmXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA)

下面的C_k可以理解为分类问题的类别：对数据集D计算关于类别C的信息熵：

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=Y2NjNTg5ZGU1MzhmMjVlMWU0ZjA1MDFlNjFmODI4ZDRfMmdKb29TdWJmM2FSMkJYbXRWdG1TS1NaNXhieU1JSXBfVG9rZW46TkVUNGJFYTNGbzJGekx4YmV3ZmNTNEdKbnBlXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA)

> 假设事件A发生概率为1，则事件A的信息量为0

> 信息论中的定义是以2为底的，这样在p=0.5的时候熵为1，计算机中一般使用e为底，两者只是差了一个常数项的乘积

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MzEzYTc3MWUyNjJjMjU2MGFiZjNhOGNiNGJkZWMwNThfZkpZOFNrOVA1YjYxMjNub2tEUFJmRTByME1VNnZnZEdfVG9rZW46Vnp1TmJUN2ZJb1RxSDh4bkw0NWNWb2hJbjdlXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA)

### 相对熵（KL散度）

衡量随机变量X的两个单独概率分布：P(x)[真实分布]， Q(x)[预测分布] 的分布差异

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=M2RmNTdhMmU5MWM3ZTNmNDUwNmQyNDE4MzkzN2U4MzBfTml2ZFBHNEpKa1owaVM1N0p3T1hndm9lOWJMV2VpaEtfVG9rZW46WlNlWWJobGNTb0dpNW94RUpGUmNObTRkbmllXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA)

> $$p||q$$表示用分布q拟合分布p

上式变形可以得到：

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YWNkY2NjMGI2OWVhNzFhOTI5OWU0Y2M3M2VhNzc4ODVfM0l1czdSY3dXSlhqdExLZURFdEE1T2hzdHlYTkRkcW1fVG9rZW46RGpIT2JqSGtFb2VtWWZ4UWd0aWNTOHQ5bkRkXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA)

前一部分是真实分布P的信息熵的相反数，是固定不变的。所以优化预测分布是优化后一部分，p和q的交叉熵。

继续变形可以得到：

$$KL(p||q) = pq的交叉熵 - p的信息熵 = H(p, q) - H(p)$$

PS：KL散度是不对称的，$$KL(p||q)$$越小，表示分布q拟合分布p拟合得越好

### 交叉熵

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDg3YzZlZDliNWExYTAxNzExMmM4OGVlYjBjOWY0ZWVfSkFBTWwzY3NkTGw0azVrMk1SMEs4SXpjVUhNTU9CWU9fVG9rZW46RDJscmIxZk43b2ozS0d4UEZWRGNXRUZkbmpsXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA)

## 决策树里面的

### 条件熵

条件熵定义为：A条件划分下得到的子集的信息熵加权和，加的权是子集在集合中的比例

在A条件下划分为n个子集D_i 的条件熵计算公式如下：

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZmU5ZDNiYTZlN2RhNmZiMmI5NjlkMTcyNTQwOTQxODVfcXpESG1ZSGVaT25ZV1FoQVJMdXA3Z0Z6R3ZWVGVmNENfVG9rZW46T2E2aGJBeERGb2Y4WEd4eENsaGNTMm4zbnBmXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA)

### 信息增益/互信息

信息增益（Information Gain，也被称为互信息）：定义特征A给数据集D带来的信息增益为 集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZGY4YTkwYWNmNDdhZTY3NmY5ODVhYWE0NmRiMzNhZmJfaks3b200N1lZT2dlWnh0dW1obTJuYTZDeGY2R2VNS3ZfVG9rZW46RnlRWmJmWTAzb2oyUHR4ZDdlVGNQblllbm1CXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA)

- 根据条件熵的性质，在给定数据集和类别下[H(D)固定]，特征A划分的子集越多，条件熵越小，信息增益越大；
  -  因此，特征取值越多，信息增益更容易越大，所以说信息增益偏向于取值较多的特征

> 如何理解特征A划分的子集越多，条件熵越小？**可以用极限的思想理解：**
>
> 假设数据集D的样本量为20，用其中一个特征A来划分数据集
>
> 划分子集最少的极限情况：假设特征A无法将数据集划分为子集[所有数据的特征A都是一样的]，这样知道特征A之后，用特征A来划分数据集，数据集D保持原状，此时条件熵H(D|A) = 1 * H(D)，信息增益为0
>
> 划分子集最多的极限情况：假设特征A的值可以将数据集D划分为20个子集[特征A有20个取值，刚好数据集D中每个样本特征A的取值不同]，那么每个子集都仅含有一个样本，此时每个子集条件熵H(D|A) = sum(1/20 * 0) = 0 [因为log1 = 1]，信息增益为H(D)

### 信息增益率

为了解决信息增益偏向于取值较多的特征这一问题，信息增益将特征取值情况考虑进去：

**信息增益率 = 信息增益 / 特征熵**

-  𝑋 的特征熵：特征 𝑋 取值的熵，衡量特征 𝑋 的不确定性；其实就是将X视为数据集，将取值视为类别的信息熵
- 特征取值越丰富多样，特征熵越高

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MjlkM2IxNTM3N2Q3MWEyMzk0MjhhY2NlM2IzNzgwN2JfNTVaMmY4cVJhUmM0Vk5WWjBWcUFjRFBIdzVBT0s5ZmNfVG9rZW46TkhSdmJWaXAyb2tuSHd4clExSmMxaHpLbjdZXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA)