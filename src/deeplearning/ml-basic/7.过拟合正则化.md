---
title: 过拟合[正则化]
order: 7

copyright: <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0协议</a>
---

过拟合：模型学习能力太强，学到了样本数据不具备普遍性的特征

泛化性：模型应用到新样本的能力

一般解决方法：

- 特征选择：丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如**PCA**）
- 正则化：保留所有特征，但是减少参数量，降低模型复杂度，从而防止过拟合
- 得到更多数据：增大数据量、数据增强

## 正则化

原理：通过在损失函数中加入额外的约束或惩罚项，控制模型的复杂度，从而提高模型在新数据上的泛化能力

很多地方都说到正则化，但其实具体怎么用，用在哪里是不一样的：

- 将正则化项加在代价函数上：目标函数 = 代价函数 +  λ 正则化项，这里 λ是正则化系数
- 优化器上对参数的更新的时候减去一个正则化项，也叫**权重衰减**

本质上两者其实是等价的，都是可以减少参数量，降低模型复杂度，从而防止过拟合

### L1正则化

定义：在损失函数中添加参数的 L1 范数（元素/参数绝对值之和）乘上正则化系数的正则化项：$$\lambda \sum_{j=1}^{n} |W_j|$$

求导：求导后为符号函数$$sign(W_i)$$

作用：使得**参数稀疏[权重稀疏]**，有**特征选择**的效果。保留重要特征，不重要特征权重为0

#### 具体分析

因为求导后为符号函数$$sign(W_i)$$，每次更新都会向零靠近，多次迭代后某些参数会变为0

更正：不用除以样本数量

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MGI3NGFhNmUyOGE1ZmI2ZDhmZGJlY2I0NGVjZDk4N2JfbVNtYXU1SzM3dDhQZTZMOVlVZVVJU3pLakdwem8zN1VfVG9rZW46TUk0RmJ5TDdQb2J5bmR4N2hRcmNXWGlQbm9lXzE3NDI4OTk2NzA6MTc0MjkwMzI3MF9WNA)

### L2正则化

定义：在损失函数中添加参数的 L2 范数（参数平方和开平方根）乘上正则化系数的的正则化项：$$\lambda \sum_{j=1}^{n} ||W_j||_2^2 = \lambda \sum_{j=1}^{n} (W_j)^2$$

L2范数：$$||W||_2 = \sqrt{W^2}$$

求导：求导后为参数本身$$W_j$$，每次更新会减小[根据学习率]，接近0但不会变为严格的0

作用：**缩小权重[权重平滑]** 。选择更多的特征，缩小权重数值，使得参数估计更加稳定

#### 具体分析

更正：不用除以样本数量

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZGMzZmUxNDgxZjFkMGRjZjBhYWRkMWE1MWMxZDBkOTRfVkZXZjVlallkYXU5N2k0RGZKcEVJTGJGa1hxOEdYN25fVG9rZW46QnJvTGI2SlM3b0dPaDh4UE9lQ2NtVjUwbkZnXzE3NDI4OTk2NzA6MTc0MjkwMzI3MF9WNA)

## DropOut

训练的时候每次随机[keep_prob]一部分神经元不参与参数更新，这些神经元的输出被设置为0

测试的时候关闭

## 增大数据量、数据增强

### Mixup等方法

## Early Stop

用验证集验证，验证集损失上升时停止训练

## 归一化层 Batch Normalization

BN中两个可学习参数相当于引入了噪声、额外的模型规则，让模型难以过拟合训练数据；

同时BN因为解决了内部协变量偏移问题，可以让模型学习效果变好，从而提高模型泛化能力