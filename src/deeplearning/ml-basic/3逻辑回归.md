---
title: 逻辑回归
order: 3

copyright: <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0协议</a>
---

对应[吴恩达机器学习P31-35](https://www.bilibili.com/video/BV1Bq421A74G?p=35&vd_source=5a405136097f34501ccd7ddbfdb09200)

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ODlmMjU3ZGU1MTViZDlhMjA2ZjgxOTAwOTdmYzQ4YmNfbGE1T3U1RGlhalBjclpycXNZcHZLcUFhTTJha0lMQ3ZfVG9rZW46Ukh3NWJZcVozb2xlcUR4WXVSc2NvS3ZmbkJlXzE3NDI4OTkzODY6MTc0MjkwMjk4Nl9WNA)

逻辑回归用于解决分类问题

## 模型建模

在现线性回归$$z = wx + b$$的基础上使用sigmoid函数$$\sigma(z) = \frac{1}{1 + e^{-z}}$$来建模概率分布，也就是将线性模型的输出变为概率：

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ODM0Yjk4M2Q5MWU4ZWJjYzUxM2VkYmE5OWZmNzRkZTVfR2VXMUJ3Tms3NU5GTGsyNkZLMEtjdUI5MWlqQkpXeDJfVG9rZW46RHlsWWJ1aVJMb2IwS2d4dkRvdGNiVVlBbnhnXzE3NDI4OTkzODY6MTc0MjkwMjk4Nl9WNA)

将模型输出$$f_{w,b}(x)$$理解为给定输入$$x$$，给定模型参数$$w，b$$时预测得到$$y = 1$$的概率

要得到最后的预测结果，需要设置一个决策阈值，如果使用0.5作为阈值：

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YmQzMmJkNDExYjhmYTAxOWIyZDExZjkwNGE5OTUwZTBfSHFKS2pROGdPeThoQ2tCTkxQSW1lU2JJRm5lMXJDOUVfVG9rZW46RUdpVWIzcFNmb3RJZDd4Y05iWGNGY0VmbldlXzE3NDI4OTkzODY6MTc0MjkwMjk4Nl9WNA)

为什么称线性方程$$z = wx + b$$为决策边界： 

> $$f_{w,b}(x) >= 0.5$$，即$$\sigma(z) >= 0.5$$，在$$z >= 0$$的时候满足，又因为$$z = wx + b$$，所以
>
> ![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YjE2MzIxOTIzMTEyNDM3ZTMxZmI4ZWM3MzNmOTNlZjdfUkRxMmdrRFY1QmRjZjFCWGhWTFVrWnNieHR5U2FOOEpfVG9rZW46V2hJM2J6dkZtb2U3SE14aW5ZMWNvSFdCbkRnXzE3NDI4OTkzODY6MTc0MjkwMjk4Nl9WNA)

## 损失函数

使用交叉熵损失：CE-Loss

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YThhM2M4MjIwMzQ4YzRiNjhmMjZmNThkYzM0MzQ0ZGRfa1JNWDBPNGY4WVVORWVjMlI4UnRFNjB0Vms0bHlXcXVfVG9rZW46Qm9WQmJTdHh6b2Q2VUt4cDBqV2NmM05wblZlXzE3NDI4OTkzODY6MTc0MjkwMjk4Nl9WNA)

目标函数是：

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjUzZDk5YjIwNDI0MzZhMmIyNjQ4ZTg5ZGZmODJhZjZfWkJlcFJReWZEV0NKRlo2SU5Ld25zbnhGUnhBY3VBQ25fVG9rZW46WlpNMGJUc29Vb21LYmN4d2lLMGNIS09abmlmXzE3NDI4OTkzODY6MTc0MjkwMjk4Nl9WNA)

其中， $$m $$ 是训练样本数量，输入为 $$x^i，y^i$$ 

 $$f_{w,b}(x^i) = \frac{1}{1 + e^{-z_i}}$$ 

 $$z_i = w x^i + b$$ 

### 梯度计算

$$L$$是损失函数loss，$$f_i$$是 $$f_{w,b}(x^i)$$ 

$$\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i = 0}^{m - 1} (\frac{\partial L}{\partial f_i}  \frac{\partial f_i}{\partial z_i} \frac{\partial z_i}{\partial w_j}) $$

其中，

交叉熵求导：$$\frac{\partial L}{\partial f_i} = -\frac{y_i}{f_i} + \frac{1-y_i}{1-f_i}$$

sigmoid函数求导：$$\frac{\partial f_i}{\partial z_i} = f_i(1-f_i)$$

线性方程求导：$$\frac{\partial z_i}{\partial w_j} = x_j^i$$

上面三个相乘，化简得到：

$$\frac{\partial L}{\partial w_j} = \frac{1}{m} \sum_{i = 0}^{m - 1} (f_i - y^i)x^i_j $$

同理可得：

$$\frac{\partial L}{\partial b} = \frac{1}{m} \sum_{i = 0}^{m - 1} (f_i - y^i) $$

所以其实表现形式和线性回归是一样的

## 相关面试问题

**为什么逻辑回归用sigmoid函数？**

> 因为逻辑回归要预测值是分类概率，需要将输出约束到[0, 1]之间
>
> sigmoid函数连续，单调递增，计算导数方便

**为什么逻辑回归用交叉熵损失？**

1. 因为交叉熵减去真实样本的信息熵等于预测概率分布和真实样本分布的KL散度，最小化交叉熵就是最小化KL散度，目的是得到更接近真实样本的概率分布
2. 这个损失函数是凸的，有全局最优解

**为什么不能用MSE作为损失函数？**

1. 逻辑回归是分类问题，对概率分布建模，不是对距离建模(MSE是计算欧式距离的），所以使用交叉熵损失
2. 如果逻辑回归中使用MSE则得到的损失函数是非凸的（non-convex），有多个局部最优解，无法做最优化求解
3. Sigmoid函数和MSE一起使用容易出现梯度消失（当预测值接近于1/0时，会导致梯度接近0）

**凸函数是什么？**

凸函数的局部最优解必然是全局最优解

证明函数为凸函数，只要证明其二阶导恒大于等于0；如果不是恒大于等于0，则为非凸函数