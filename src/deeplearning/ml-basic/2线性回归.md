---
title: 线性回归
order: 2

copyright: <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0协议</a>

---

线性模型用于回归问题

## 基础

使用线性回归$$f_{w,b}(x^i) = wx^i + b$$建模

```Bash
def _f(self, x, a, b):
    """一元线性函数"""
    return x * a + b
```

损失函数使用MSE(Mean Squared Error，均方误差）：

$$Loss = \frac{1}{m}\sum_{i=0}^m(y_i - \bar{y_i})^2$$ 

其中： $$y_i = f_{w,b}(x^i)$$  

代码表示为： `np.mean((y_true - y_pred)**2)`

### 梯度

$$L$$是损失函数loss，$$f_i$$是 $$f_{w,b}(x^i)$$ 

$$\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i = 0}^{m - 1}(f_i - y^i)x^i_j $$

```
d_w=np.mean((self.x * self.a + self.b - self.y) * self.x)
```

$$\frac{\partial L}{\partial b} = \frac{1}{m} \sum_{i = 0}^{m - 1} (f_i - y^i) $$

```
d_b=np.mean(self.x * self.a + self.b - self.y)
```

## 一元线性回归 代码实现

```Python
import numpy as np
class LinearRegression():
    def __init__(self, learning_rate=0.01, max_iter=100, seed=42):
        np.random.seed(seed)
        self.lr = learning_rate
        self.max_iter = max_iter
        self.a = np.random.normal(1, 0.1) # 均值为 1、标准差为 0.1 的正态分布的随机值
        self.b = np.random.normal(1, 0.1)
        self.loss_arr = []

    def fit(self, x, y):
        self.x = x
        self.y = y
        for i in range(self.max_iter):
            self._train_step()
            loss = self.loss()
            print(loss)
            self.loss_arr.append(loss)

    def predict(self, x=None):
        if x is None:
            x = self.x
        return self.a * self.x + self.b

    def loss(self, y_true=None, y_pred=None):
        if y_true is None or y_pred is None:
            y_true = self.y
            y_pred = self.predict(self.x)
        return np.mean((y_true - y_pred) ** 2)
    
    def _calc_gradient(self):
        d_a = np.mean((self.a * self.x + self.b - self.y) * self.x)
        d_b = np.mean(self.a * self.x + self.b - self.y)
        return d_a, d_b

    def _train_step(self):
        d_a, d_b = self._calc_gradient()
        self.a = self.a - self.lr * d_a
        self.b = self.b - self.lr * d_b
        print(d_a, d_b)
        return self.a, self.b
```

### 测试

```Python
def get_data_set(data_size):
    np.random.seed(300)
    x = np.random.uniform(low = 1.0, high = 10.0, size = data_size)
    y = x * 20 + np.random.normal(0.0, 10.0, data_size)
    return x, y

def get_train_test_dataset(x, y, ratio):
    shuffle_index = np.arange(len(x))
    np.random.shuffle(shuffle_index)
    x = x[shuffle_index]
    y = y[shuffle_index]
    num = int(len(x)*ratio)
    x_train = x[:num]
    y_train = y[:num]
    x_test = x[num:]
    y_test = y[num:]
    return x_train, y_train, x_test, y_test

x, y = get_data_set(100)
x_train, y_train, x_test, y_test = get_train_test_dataset(x, y, 0.7)
model = LinearRegression(learning_rate=0.001, max_iter=100, seed=10)
model.fit(x_train, y_train)

import matplotlib.pyplot as plt
plt.scatter(x, y, marker='x', c='red')
plt.show()

plt.scatter(np.arange(len(model.loss_arr)), model.loss_arr, marker='o', c='green')
plt.show()
print("w = ",model.a," ","b = ",model.b)
```

## 多元线性回归 代码实现

![img](https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=OTA2NTY5OWYwZmE1MzNjZDU5NTBkNWQ4ZDVhYWYzYmZfN3FTdUZ0a21TTXppeEJIN25WOXphcGs0dE9mQ1JlUzdfVG9rZW46VWJ3QmJWbW1nb2FlQWF4aGtYVGN6WDZ2bk5lXzE3NDI4OTkzMTE6MTc0MjkwMjkxMV9WNA)