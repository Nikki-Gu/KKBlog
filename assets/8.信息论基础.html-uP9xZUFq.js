import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as l,o as t,d as e,a}from"./app-DQsaqGl9.js";const i={},n=e(`<h2 id="交叉熵损失的理解" tabindex="-1"><a class="header-anchor" href="#交叉熵损失的理解"><span>交叉熵损失的理解</span></a></h2><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MTEzYjgxN2UzN2RiMTFmZjk0ZmMzOTEyMzllMmY1OGFfYVY3TDl4eVRnRTJaWHVqYnNXWnphMHgwZlJFeHBkdURfVG9rZW46UkZaSmJKNnZUb2NBZkV4RmtsYWMzVHlXbmVsXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h3 id="信息熵" tabindex="-1"><a class="header-anchor" href="#信息熵"><span>信息熵</span></a></h3><ul><li>entropy：a measure of impurity</li><li>用来度量信息的混乱程度、不确定度。不确定性越大，信息熵越大</li></ul><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=NDE1Nzk1MzRjYTZhNmVmMDk3MzA1NTg1MTBiZjNjODRfOER4QU9QcTUxOHVSczZRaHdoenVMV205WnBuMUNTNTlfVG9rZW46S1l5MWJmY29Eb2RLZkR4NGJFbGNGWWFHblRmXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>下面的C_k可以理解为分类问题的类别：对数据集D计算关于类别C的信息熵：</p><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=Y2NjNTg5ZGU1MzhmMjVlMWU0ZjA1MDFlNjFmODI4ZDRfMmdKb29TdWJmM2FSMkJYbXRWdG1TS1NaNXhieU1JSXBfVG9rZW46TkVUNGJFYTNGbzJGekx4YmV3ZmNTNEdKbnBlXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><blockquote><p>假设事件A发生概率为1，则事件A的信息量为0</p></blockquote><blockquote><p>信息论中的定义是以2为底的，这样在p=0.5的时候熵为1，计算机中一般使用e为底，两者只是差了一个常数项的乘积</p></blockquote><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MzEzYTc3MWUyNjJjMjU2MGFiZjNhOGNiNGJkZWMwNThfZkpZOFNrOVA1YjYxMjNub2tEUFJmRTByME1VNnZnZEdfVG9rZW46Vnp1TmJUN2ZJb1RxSDh4bkw0NWNWb2hJbjdlXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h3 id="相对熵-kl散度" tabindex="-1"><a class="header-anchor" href="#相对熵-kl散度"><span>相对熵（KL散度）</span></a></h3><p>衡量随机变量X的两个单独概率分布：P(x)[真实分布]， Q(x)[预测分布] 的分布差异</p><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=M2RmNTdhMmU5MWM3ZTNmNDUwNmQyNDE4MzkzN2U4MzBfTml2ZFBHNEpKa1owaVM1N0p3T1hndm9lOWJMV2VpaEtfVG9rZW46WlNlWWJobGNTb0dpNW94RUpGUmNObTRkbmllXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><blockquote><p class="katex-block"><span class="katex-error" title="ParseError: KaTeX parse error: Can&#39;t use function &#39;$&#39; in math mode at position 6: 
p||q$̲$表示用分布q拟合分布p
" style="color:#cc0000;"> p||q$$表示用分布q拟合分布p </span></p></blockquote><p>上式变形可以得到：</p><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YWNkY2NjMGI2OWVhNzFhOTI5OWU0Y2M3M2VhNzc4ODVfM0l1czdSY3dXSlhqdExLZURFdEE1T2hzdHlYTkRkcW1fVG9rZW46RGpIT2JqSGtFb2VtWWZ4UWd0aWNTOHQ5bkRkXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>前一部分是真实分布P的信息熵的相反数，是固定不变的。所以优化预测分布是优化后一部分，p和q的交叉熵。</p><p>继续变形可以得到：</p>`,18),c=a("p",{class:"katex-block"},[a("span",{class:"katex-display"},[a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[a("semantics",null,[a("mrow",null,[a("mi",null,"K"),a("mi",null,"L"),a("mo",{stretchy:"false"},"("),a("mi",null,"p"),a("mi",{mathvariant:"normal"},"∣"),a("mi",{mathvariant:"normal"},"∣"),a("mi",null,"q"),a("mo",{stretchy:"false"},")"),a("mo",null,"="),a("mi",null,"p"),a("mi",null,"q"),a("mtext",null,"的交叉熵"),a("mo",null,"−"),a("mi",null,"p"),a("mtext",null,"的信息熵"),a("mo",null,"="),a("mi",null,"H"),a("mo",{stretchy:"false"},"("),a("mi",null,"p"),a("mo",{separator:"true"},","),a("mi",null,"q"),a("mo",{stretchy:"false"},")"),a("mo",null,"−"),a("mi",null,"H"),a("mo",{stretchy:"false"},"("),a("mi",null,"p"),a("mo",{stretchy:"false"},")")]),a("annotation",{encoding:"application/x-tex"}," KL(p||q) = pq的交叉熵 - p的信息熵 = H(p, q) - H(p) ")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K"),a("span",{class:"mord mathnormal"},"L"),a("span",{class:"mopen"},"("),a("span",{class:"mord mathnormal"},"p"),a("span",{class:"mord"},"∣∣"),a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),a("span",{class:"mclose"},")"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},"="),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.8778em","vertical-align":"-0.1944em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"pq"),a("span",{class:"mord cjk_fallback"},"的交叉熵"),a("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),a("span",{class:"mbin"},"−"),a("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.8778em","vertical-align":"-0.1944em"}}),a("span",{class:"mord mathnormal"},"p"),a("span",{class:"mord cjk_fallback"},"的信息熵"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},"="),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.08125em"}},"H"),a("span",{class:"mopen"},"("),a("span",{class:"mord mathnormal"},"p"),a("span",{class:"mpunct"},","),a("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),a("span",{class:"mclose"},")"),a("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),a("span",{class:"mbin"},"−"),a("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.08125em"}},"H"),a("span",{class:"mopen"},"("),a("span",{class:"mord mathnormal"},"p"),a("span",{class:"mclose"},")")])])])])],-1),m=e('<p>PS：KL散度是不对称的，$$KL(p||q)$$越小，表示分布q拟合分布p拟合得越好</p><h3 id="交叉熵" tabindex="-1"><a class="header-anchor" href="#交叉熵"><span>交叉熵</span></a></h3><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDg3YzZlZDliNWExYTAxNzExMmM4OGVlYjBjOWY0ZWVfSkFBTWwzY3NkTGw0azVrMk1SMEs4SXpjVUhNTU9CWU9fVG9rZW46RDJscmIxZk43b2ozS0d4UEZWRGNXRUZkbmpsXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h2 id="决策树里面的" tabindex="-1"><a class="header-anchor" href="#决策树里面的"><span>决策树里面的</span></a></h2><h3 id="条件熵" tabindex="-1"><a class="header-anchor" href="#条件熵"><span>条件熵</span></a></h3><p>条件熵定义为：A条件划分下得到的子集的信息熵加权和，加的权是子集在集合中的比例</p><p>在A条件下划分为n个子集D_i 的条件熵计算公式如下：</p><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZmU5ZDNiYTZlN2RhNmZiMmI5NjlkMTcyNTQwOTQxODVfcXpESG1ZSGVaT25ZV1FoQVJMdXA3Z0Z6R3ZWVGVmNENfVG9rZW46T2E2aGJBeERGb2Y4WEd4eENsaGNTMm4zbnBmXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h3 id="信息增益-互信息" tabindex="-1"><a class="header-anchor" href="#信息增益-互信息"><span>信息增益/互信息</span></a></h3><p>信息增益（Information Gain，也被称为互信息）：定义特征A给数据集D带来的信息增益为 集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：</p><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZGY4YTkwYWNmNDdhZTY3NmY5ODVhYWE0NmRiMzNhZmJfaks3b200N1lZT2dlWnh0dW1obTJuYTZDeGY2R2VNS3ZfVG9rZW46RnlRWmJmWTAzb2oyUHR4ZDdlVGNQblllbm1CXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><ul><li>根据条件熵的性质，在给定数据集和类别下[H(D)固定]，特征A划分的子集越多，条件熵越小，信息增益越大； <ul><li>因此，特征取值越多，信息增益更容易越大，所以说信息增益偏向于取值较多的特征</li></ul></li></ul><blockquote><p>如何理解特征A划分的子集越多，条件熵越小？<strong>可以用极限的思想理解：</strong></p><p>假设数据集D的样本量为20，用其中一个特征A来划分数据集</p><p>划分子集最少的极限情况：假设特征A无法将数据集划分为子集[所有数据的特征A都是一样的]，这样知道特征A之后，用特征A来划分数据集，数据集D保持原状，此时条件熵H(D|A) = 1 * H(D)，信息增益为0</p><p>划分子集最多的极限情况：假设特征A的值可以将数据集D划分为20个子集[特征A有20个取值，刚好数据集D中每个样本特征A的取值不同]，那么每个子集都仅含有一个样本，此时每个子集条件熵H(D|A) = sum(1/20 * 0) = 0 [因为log1 = 1]，信息增益为H(D)</p></blockquote><h3 id="信息增益率" tabindex="-1"><a class="header-anchor" href="#信息增益率"><span>信息增益率</span></a></h3><p>为了解决信息增益偏向于取值较多的特征这一问题，信息增益将特征取值情况考虑进去：</p><p><strong>信息增益率 = 信息增益 / 特征熵</strong></p><ul><li>𝑋 的特征熵：特征 𝑋 取值的熵，衡量特征 𝑋 的不确定性；其实就是将X视为数据集，将取值视为类别的信息熵</li><li>特征取值越丰富多样，特征熵越高</li></ul><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MjlkM2IxNTM3N2Q3MWEyMzk0MjhhY2NlM2IzNzgwN2JfNTVaMmY4cVJhUmM0Vk5WWjBWcUFjRFBIdzVBT0s5ZmNfVG9rZW46TkhSdmJWaXAyb2tuSHd4clExSmMxaHpLbjdZXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure>',18),p=[n,c,m];function o(r,d){return t(),l("div",null,p)}const M=s(i,[["render",o],["__file","8.信息论基础.html.vue"]]),g=JSON.parse(`{"path":"/deeplearning/ml-basic/8.%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80.html","title":"信息论基础","lang":"zh-CN","frontmatter":{"title":"信息论基础","order":8,"copyright":"<a href=\\"https://creativecommons.org/licenses/by-nc/4.0/\\">CC BY-NC 4.0协议</a>","description":"交叉熵损失的理解 imgimg 信息熵 entropy：a measure of impurity 用来度量信息的混乱程度、不确定度。不确定性越大，信息熵越大 imgimg 下面的C_k可以理解为分类问题的类别：对数据集D计算关于类别C的信息熵： imgimg 假设事件A发生概率为1，则事件A的信息量为0 信息论中的定义是以2为底的，这样在p=0.5的...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/KKBlog/deeplearning/ml-basic/8.%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80.html"}],["meta",{"property":"og:site_name","content":"KK's Blog"}],["meta",{"property":"og:title","content":"信息论基础"}],["meta",{"property":"og:description","content":"交叉熵损失的理解 imgimg 信息熵 entropy：a measure of impurity 用来度量信息的混乱程度、不确定度。不确定性越大，信息熵越大 imgimg 下面的C_k可以理解为分类问题的类别：对数据集D计算关于类别C的信息熵： imgimg 假设事件A发生概率为1，则事件A的信息量为0 信息论中的定义是以2为底的，这样在p=0.5的..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MTEzYjgxN2UzN2RiMTFmZjk0ZmMzOTEyMzllMmY1OGFfYVY3TDl4eVRnRTJaWHVqYnNXWnphMHgwZlJFeHBkdURfVG9rZW46UkZaSmJKNnZUb2NBZkV4RmtsYWMzVHlXbmVsXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-25T10:49:33.000Z"}],["meta",{"property":"article:author","content":"KK"}],["meta",{"property":"article:modified_time","content":"2025-03-25T10:49:33.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"信息论基础\\",\\"image\\":[\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MTEzYjgxN2UzN2RiMTFmZjk0ZmMzOTEyMzllMmY1OGFfYVY3TDl4eVRnRTJaWHVqYnNXWnphMHgwZlJFeHBkdURfVG9rZW46UkZaSmJKNnZUb2NBZkV4RmtsYWMzVHlXbmVsXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=NDE1Nzk1MzRjYTZhNmVmMDk3MzA1NTg1MTBiZjNjODRfOER4QU9QcTUxOHVSczZRaHdoenVMV205WnBuMUNTNTlfVG9rZW46S1l5MWJmY29Eb2RLZkR4NGJFbGNGWWFHblRmXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=Y2NjNTg5ZGU1MzhmMjVlMWU0ZjA1MDFlNjFmODI4ZDRfMmdKb29TdWJmM2FSMkJYbXRWdG1TS1NaNXhieU1JSXBfVG9rZW46TkVUNGJFYTNGbzJGekx4YmV3ZmNTNEdKbnBlXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MzEzYTc3MWUyNjJjMjU2MGFiZjNhOGNiNGJkZWMwNThfZkpZOFNrOVA1YjYxMjNub2tEUFJmRTByME1VNnZnZEdfVG9rZW46Vnp1TmJUN2ZJb1RxSDh4bkw0NWNWb2hJbjdlXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=M2RmNTdhMmU5MWM3ZTNmNDUwNmQyNDE4MzkzN2U4MzBfTml2ZFBHNEpKa1owaVM1N0p3T1hndm9lOWJMV2VpaEtfVG9rZW46WlNlWWJobGNTb0dpNW94RUpGUmNObTRkbmllXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YWNkY2NjMGI2OWVhNzFhOTI5OWU0Y2M3M2VhNzc4ODVfM0l1czdSY3dXSlhqdExLZURFdEE1T2hzdHlYTkRkcW1fVG9rZW46RGpIT2JqSGtFb2VtWWZ4UWd0aWNTOHQ5bkRkXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDg3YzZlZDliNWExYTAxNzExMmM4OGVlYjBjOWY0ZWVfSkFBTWwzY3NkTGw0azVrMk1SMEs4SXpjVUhNTU9CWU9fVG9rZW46RDJscmIxZk43b2ozS0d4UEZWRGNXRUZkbmpsXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZmU5ZDNiYTZlN2RhNmZiMmI5NjlkMTcyNTQwOTQxODVfcXpESG1ZSGVaT25ZV1FoQVJMdXA3Z0Z6R3ZWVGVmNENfVG9rZW46T2E2aGJBeERGb2Y4WEd4eENsaGNTMm4zbnBmXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZGY4YTkwYWNmNDdhZTY3NmY5ODVhYWE0NmRiMzNhZmJfaks3b200N1lZT2dlWnh0dW1obTJuYTZDeGY2R2VNS3ZfVG9rZW46RnlRWmJmWTAzb2oyUHR4ZDdlVGNQblllbm1CXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MjlkM2IxNTM3N2Q3MWEyMzk0MjhhY2NlM2IzNzgwN2JfNTVaMmY4cVJhUmM0Vk5WWjBWcUFjRFBIdzVBT0s5ZmNfVG9rZW46TkhSdmJWaXAyb2tuSHd4clExSmMxaHpLbjdZXzE3NDI4OTk2OTE6MTc0MjkwMzI5MV9WNA\\"],\\"dateModified\\":\\"2025-03-25T10:49:33.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"KK\\",\\"url\\":\\"https://github.com/Nikki-Gu\\"}]}"]]},"headers":[{"level":2,"title":"交叉熵损失的理解","slug":"交叉熵损失的理解","link":"#交叉熵损失的理解","children":[{"level":3,"title":"信息熵","slug":"信息熵","link":"#信息熵","children":[]},{"level":3,"title":"相对熵（KL散度）","slug":"相对熵-kl散度","link":"#相对熵-kl散度","children":[]},{"level":3,"title":"交叉熵","slug":"交叉熵","link":"#交叉熵","children":[]}]},{"level":2,"title":"决策树里面的","slug":"决策树里面的","link":"#决策树里面的","children":[{"level":3,"title":"条件熵","slug":"条件熵","link":"#条件熵","children":[]},{"level":3,"title":"信息增益/互信息","slug":"信息增益-互信息","link":"#信息增益-互信息","children":[]},{"level":3,"title":"信息增益率","slug":"信息增益率","link":"#信息增益率","children":[]}]}],"git":{"createdTime":1742899773000,"updatedTime":1742899773000,"contributors":[{"name":"Nikki-Gu","email":"394632208@qq.com","commits":1}]},"readingTime":{"minutes":3.16,"words":947},"filePathRelative":"deeplearning/ml-basic/8.信息论基础.md","localizedDate":"2025年3月25日","autoDesc":true}`);export{M as comp,g as data};
