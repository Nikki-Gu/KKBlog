import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,o as a,d as o}from"./app-DQsaqGl9.js";const t={},l=o(`<p>二分类：sigmoid，BCE Loss</p><p>多分类：softmax，CE Loss</p><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YmVkOWVjNjA0YTE3YjJiMWE2ZTNkYThiMWQyMjM2YWRfRlFtYjlGb1FTSnJ0MXlCbFo1OUNVRE1yc0dqUjdXM0pfVG9rZW46WGtmM2JObmlhb3Z6czN4VDcyZmNsWEg5bnNoXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>Pytorch中的使用这两个损失函数</p><div class="language-JSON line-numbers-mode" data-ext="JSON" data-title="JSON"><pre class="language-JSON"><code>// BCE Loss 输入需要先手动进行sigmoid计算再输入
torch.nn.BCELoss()

// softmax + CE Loss
torch.nn.CrossEntropyLoss()
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>最后一层全连接层的神经元个数不同，含义不同。二分类是一个神经元输出概率值（是/不是目标），多分类是多个神经元，每个神经元是一个分布</p><h2 id="sigmoid" tabindex="-1"><a class="header-anchor" href="#sigmoid"><span>Sigmoid</span></a></h2><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjQ0YjI4MTg3Y2I5MGExMTYwYmJkNGNmMmNjNzQxZWRfdDk5R3ZoTlNNSTFuUDVhRmJFMFBLclh6QkNBQXJoTEhfVG9rZW46T1dpM2JOTEpCb0Z2aFd4QjZvZ2MwbTVvbkRnXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>Sigmoid 函数具有以下特点：</p><ul><li>单调递增：输入值增加时，Sigmoid 函数的输出值也会增加。</li><li>连续可导：Sigmoid 函数在定义域内处处可导，便于使用梯度下降等优化算法进行训练。</li><li>易于计算：Sigmoid 函数的计算相对简单，只涉及指数函数和除法操作。</li></ul><p>缺点：</p><ul><li>梯度消失：函数的输出并不以零为中心；在输入值较大或较小时，梯度可能会消失</li></ul><p>求导：</p><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YzkyZmQyNjgwMzU0ZGZmMDM4ZjIxY2E5ODZhZjFhN2NfWThSMDh1MjRwVTk4dlpIVFM1U1NZUlFlV3NUWE4zbEpfVG9rZW46THk5VGI3R0Nqb1BxYkd4UGhDSGNoNGh3blZkXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h2 id="softmax" tabindex="-1"><a class="header-anchor" href="#softmax"><span>Softmax</span></a></h2><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjZlYjIyMTg4YWI4MDRkYjBhNDFiN2M5ZGFmMjVkMTNfZmV4OWZ4eDFjOFliZGJFU1VMVXNFYUc0TmFHb2diWFJfVG9rZW46UkI3UWJOWGF4b2x2YnR4ZFFYemNoRm1Wbnh6XzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>Softmax 函数具有以下特点：</p><ul><li>将输入值映射到一个概率分布：常用于对多分类问题进行建模和预测</li><li>相对大小：Softmax 函数保持了输入值之间的相对大小关系；对于输出的相对大小非常敏感，可以将差距大的数值距离拉的更大</li><li>平滑性：Softmax 函数的输出是连续的，并且对输入值的变化相对平滑，因此在训练过程中可以更好地利用梯度信息</li></ul><h3 id="求导" tabindex="-1"><a class="header-anchor" href="#求导"><span>求导</span></a></h3><p>指对xj求导，分为两种情况讨论：</p><ul><li>i=j是指 softmax(xi)对xi求导</li><li>i不等于j是指 softmax(xi)对xj求导</li></ul><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MGQ3YzZhNTZmNTUzZTdjNzQ0ZWNhNTJhYjRjYjFmMGNfVnozYWpSWUYwU21nalhFblVaa3FuTkt2Z2VjWmJ4RFVfVG9rZW46Szd0WmJOUXVSb3VyMlJ4U1JhSmNoM2tzbk1oXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h3 id="代码实现" tabindex="-1"><a class="header-anchor" href="#代码实现"><span>代码实现</span></a></h3><div class="language-Python line-numbers-mode" data-ext="Python" data-title="Python"><pre class="language-Python"><code>def softmax(x):
    exp_x = torch.exp(x)
    sum_exp_x = torch.sum(exp_x, dim = 1, keepdim=True)
    return exp_x / sum_exp_x
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="上溢出和下溢出问题" tabindex="-1"><a class="header-anchor" href="#上溢出和下溢出问题"><span>上溢出和下溢出问题</span></a></h3><p>上溢出：xi指数运算的取值过大，超出精度表示范围</p><p>下溢出：xi指数运算的取值很小，分母变为0</p><p>解决溢出问题有两种方法</p><ol><li><h4 id="变为计算softmax-xi-c" tabindex="-1"><a class="header-anchor" href="#变为计算softmax-xi-c"><span>变为计算softmax(xi - c)</span></a></h4></li></ol><ul><li>c是x中的最大值</li><li>分子的最大取值变为了exp(0)=1，避免了上溢出；</li><li>分母中至少会+1，避免了分母为0造成下溢出。</li></ul><div class="language-Python line-numbers-mode" data-ext="Python" data-title="Python"><pre class="language-Python"><code>def softmax_1(x):
    c, _ = torch.max(x, dim=1, keepdim=True)
    exp_x = torch.exp(x - c)
    sum_exp_x = torch.sum(exp_x, dim = 1, keepdim=True)
    return exp_x / sum_exp_x
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol><li><h4 id="log-softmax" tabindex="-1"><a class="header-anchor" href="#log-softmax"><span>Log-softmax</span></a></h4></li></ol><p>定义为：log(softmax(x))</p><p>同样需要通过softmax(xi - c)解决上溢出问题</p><p>存在下溢出问题：softmax(x)=0的时候，求log返回为-inf 下溢出</p><p>好处：减少指数运算次数，求导更方便，可以加快反向传播速度</p><h2 id="tanh、relu、leaky-relu" tabindex="-1"><a class="header-anchor" href="#tanh、relu、leaky-relu"><span>Tanh、Relu、Leaky Relu</span></a></h2><figure><img src="https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=N2YzNWRiZjIzMDlhNmU2NWVlZjA3MzM1MDNjMDMzMWRfRkgyWDZFWHJWdkVhUkRsY2U0NjlGblNRdHBYRmx2aXBfVG9rZW46WTB5Z2I5V25FbzB2WnN4RnU0QmNRWW9wblVjXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure>`,38),n=[l];function s(c,d){return a(),i("div",null,n)}const p=e(t,[["render",s],["__file","4.激活函数和损失函数.html.vue"]]),h=JSON.parse(`{"path":"/deeplearning/ml-basic/4.%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html","title":"激活函数和损失函数","lang":"zh-CN","frontmatter":{"title":"激活函数和损失函数","order":4,"copyright":"<a href=\\"https://creativecommons.org/licenses/by-nc/4.0/\\">CC BY-NC 4.0协议</a>","description":"二分类：sigmoid，BCE Loss 多分类：softmax，CE Loss imgimg Pytorch中的使用这两个损失函数 最后一层全连接层的神经元个数不同，含义不同。二分类是一个神经元输出概率值（是/不是目标），多分类是多个神经元，每个神经元是一个分布 Sigmoid imgimg Sigmoid 函数具有以下特点： 单调递增：输入值增加时...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/KKBlog/deeplearning/ml-basic/4.%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html"}],["meta",{"property":"og:site_name","content":"KK's Blog"}],["meta",{"property":"og:title","content":"激活函数和损失函数"}],["meta",{"property":"og:description","content":"二分类：sigmoid，BCE Loss 多分类：softmax，CE Loss imgimg Pytorch中的使用这两个损失函数 最后一层全连接层的神经元个数不同，含义不同。二分类是一个神经元输出概率值（是/不是目标），多分类是多个神经元，每个神经元是一个分布 Sigmoid imgimg Sigmoid 函数具有以下特点： 单调递增：输入值增加时..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YmVkOWVjNjA0YTE3YjJiMWE2ZTNkYThiMWQyMjM2YWRfRlFtYjlGb1FTSnJ0MXlCbFo1OUNVRE1yc0dqUjdXM0pfVG9rZW46WGtmM2JObmlhb3Z6czN4VDcyZmNsWEg5bnNoXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-25T10:49:33.000Z"}],["meta",{"property":"article:author","content":"KK"}],["meta",{"property":"article:modified_time","content":"2025-03-25T10:49:33.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"激活函数和损失函数\\",\\"image\\":[\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YmVkOWVjNjA0YTE3YjJiMWE2ZTNkYThiMWQyMjM2YWRfRlFtYjlGb1FTSnJ0MXlCbFo1OUNVRE1yc0dqUjdXM0pfVG9rZW46WGtmM2JObmlhb3Z6czN4VDcyZmNsWEg5bnNoXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjQ0YjI4MTg3Y2I5MGExMTYwYmJkNGNmMmNjNzQxZWRfdDk5R3ZoTlNNSTFuUDVhRmJFMFBLclh6QkNBQXJoTEhfVG9rZW46T1dpM2JOTEpCb0Z2aFd4QjZvZ2MwbTVvbkRnXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=YzkyZmQyNjgwMzU0ZGZmMDM4ZjIxY2E5ODZhZjFhN2NfWThSMDh1MjRwVTk4dlpIVFM1U1NZUlFlV3NUWE4zbEpfVG9rZW46THk5VGI3R0Nqb1BxYkd4UGhDSGNoNGh3blZkXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjZlYjIyMTg4YWI4MDRkYjBhNDFiN2M5ZGFmMjVkMTNfZmV4OWZ4eDFjOFliZGJFU1VMVXNFYUc0TmFHb2diWFJfVG9rZW46UkI3UWJOWGF4b2x2YnR4ZFFYemNoRm1Wbnh6XzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=MGQ3YzZhNTZmNTUzZTdjNzQ0ZWNhNTJhYjRjYjFmMGNfVnozYWpSWUYwU21nalhFblVaa3FuTkt2Z2VjWmJ4RFVfVG9rZW46Szd0WmJOUXVSb3VyMlJ4U1JhSmNoM2tzbk1oXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA\\",\\"https://pq18uqc90b.feishu.cn/space/api/box/stream/download/asynccode/?code=N2YzNWRiZjIzMDlhNmU2NWVlZjA3MzM1MDNjMDMzMWRfRkgyWDZFWHJWdkVhUkRsY2U0NjlGblNRdHBYRmx2aXBfVG9rZW46WTB5Z2I5V25FbzB2WnN4RnU0QmNRWW9wblVjXzE3NDI4OTk1MTA6MTc0MjkwMzExMF9WNA\\"],\\"dateModified\\":\\"2025-03-25T10:49:33.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"KK\\",\\"url\\":\\"https://github.com/Nikki-Gu\\"}]}"]]},"headers":[{"level":2,"title":"Sigmoid","slug":"sigmoid","link":"#sigmoid","children":[]},{"level":2,"title":"Softmax","slug":"softmax","link":"#softmax","children":[{"level":3,"title":"求导","slug":"求导","link":"#求导","children":[]},{"level":3,"title":"代码实现","slug":"代码实现","link":"#代码实现","children":[]},{"level":3,"title":"上溢出和下溢出问题","slug":"上溢出和下溢出问题","link":"#上溢出和下溢出问题","children":[]}]},{"level":2,"title":"Tanh、Relu、Leaky Relu","slug":"tanh、relu、leaky-relu","link":"#tanh、relu、leaky-relu","children":[]}],"git":{"createdTime":1742899773000,"updatedTime":1742899773000,"contributors":[{"name":"Nikki-Gu","email":"394632208@qq.com","commits":1}]},"readingTime":{"minutes":2.33,"words":700},"filePathRelative":"deeplearning/ml-basic/4.激活函数和损失函数.md","localizedDate":"2025年3月25日","autoDesc":true}`);export{p as comp,h as data};
